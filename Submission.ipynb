{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity Edits: AI-Powered Automated Video Editing System\n",
    "\n",
    "This notebook serves as the primary artifact for the Gravity Edits backend system. It documents the problem, design, implementation, and analysis of the autonomous video editing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition & Objective\n",
    "\n",
    "**Objective:** To automate the labor-intensive process of video editing (trimming, color grading, overlay creation, and format adaptation) using Generative AI and Computer Vision.\n",
    "\n",
    "**Problem Statement:**\n",
    "Creating engaging video content requires significant manual effort in identifying 'good' takes, cleaning up audio, applying color corrections, and re-formatting for different platforms (e.g., Shorts/Reels). Gravity Edits aims to solve this by:\n",
    "1.  Automatically transcribing and understanding video content.\n",
    "2.  Using LLMs (Gemini/Llama 3) to make editorial decisions (cuts, keepers, viral moments).\n",
    "3.  Rendering the final output programmatically using MoviePy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "\n",
    "The system processes raw video files (MP4, MOV) and Audio files. \n",
    "\n",
    "**Data Pipeline:**\n",
    "1.  **Ingestion:** Raw video files are uploaded and stored.\n",
    "2.  **Audio Extraction:** Audio is extracted from video containers.\n",
    "3.  **Transcription:** Speech-to-Text models (Faster-Whisper) generate timestamped transcripts.\n",
    "4.  **Visual Analysis:** OpenCV calculates brightness, contrast, and emotion metrics for every clip.\n",
    "5.  **Sanitization:** The 'Ghostbuster' protocol cleans phantom words and phonetic errors from the transcript before passing it to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!pip install fastapi uvicorn redis rq moviepy openai opencv-python-headless pydantic requests google-generativeai proglog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model/System Design\n",
    "\n",
    "**Architecture:**\n",
    "*   **Frontend:** React-based timeline editor using a **Normalized Coordinate System (0-1)** for perfect cross-device scaling.\n",
    "*   **Backend:** FastAPI server managing state, uploads, and orchestration.\n",
    "*   **AI Engine:** The 'Two-Stage Brain' (Inspector + Director) that separates forensic analysis from creative editing.\n",
    "*   **Renderer:** A **Hybrid Engine** (MoviePy + FFmpeg) that uses Python for asset generation and FFmpeg for crashing-proof compositing.\n",
    "\n",
    "**ML/LLM Technique:**\n",
    "*   **Dual-Agent Workflow:** We employ an 'Inspector' agent to detect hallucinations and a 'Director' agent to make creative decisions.\n",
    "*   **Wakullah Protocol V2:** A strict 6-step prompt protocol ensuring Zero-Tolerance for empty overlays and mandatory viral short generation.\n",
    "*   **RAG-like Context:** We construct a rich JSON representation of the video timeline (clips, text, visual stats, timestamps) and feed it to the Gemini 1.5 Pro model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Implementation\n",
    "\n",
    "Here we present the core backend modules responsible for the AI logic and Video Rendering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 AI Engine (`backend/ai_engine.py`)\n",
    "This module handles the orchestration of transcription, visual analysis, and LLM interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "# Imports moved to functions to prevent startup locks\n",
    "# from moviepy import VideoFileClip\n",
    "# from faster_whisper import WhisperModel\n",
    "# from deepface import DeepFace\n",
    "\n",
    "# Settings\n",
    "TEMP_AUDIO_DIR = \"processing\"\n",
    "os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)\n",
    "MODEL_SIZE = \"base\"\n",
    "\n",
    "def extract_audio(video_path):\n",
    "    # Same as before\n",
    "    base_name = os.path.basename(video_path)\n",
    "    audio_path = os.path.join(TEMP_AUDIO_DIR, f\"{base_name}.wav\")\n",
    "    if os.path.exists(audio_path): return audio_path\n",
    "    \n",
    "    from moviepy import VideoFileClip\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path, logger=None)\n",
    "    video.close()\n",
    "    return audio_path\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    print(f\"      [2/3] Transcribing Audio for {os.path.basename(audio_path)}...\")\n",
    "    \n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            [sys.executable, \"backend/audio_transcriber.py\", audio_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        stdout, stderr = process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error in transcription: {stderr}\")\n",
    "            # Fallback mock\n",
    "            return [{\n",
    "                \"start\": 0.0,\n",
    "                \"end\": 5.0,\n",
    "                \"text\": \"[Transcription Failed]\",\n",
    "                \"visual_data\": {}\n",
    "            }]\n",
    "\n",
    "        # Parse the JSON output from the script\n",
    "        # The script might print logs, but the last line should be the JSON\n",
    "        lines = stdout.strip().split('\\n')\n",
    "        result_json_str = lines[-1] \n",
    "        return json.loads(result_json_str)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run isolated transcriber: {e}\")\n",
    "        return [{\n",
    "            \"start\": 0.0,\n",
    "            \"end\": 5.0,\n",
    "            \"text\": \"[System Failure]\",\n",
    "            \"visual_data\": {}\n",
    "        }]\n",
    "\n",
    "def analyze_visuals(video_path, clips):\n",
    "    # Same as before\n",
    "    # Prepare timestamps to analyze\n",
    "    timestamps = []\n",
    "    for clip in clips:\n",
    "        timestamps.append((clip[\"start\"], clip[\"end\"]))\n",
    "    \n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Run the isolated script\n",
    "    # We pass the timestamps via stdin to the script\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            [sys.executable, \"backend/visual_analyzer.py\", video_path],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        stdout, stderr = process.communicate(input=json.dumps(timestamps))\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error in visual analysis: {stderr}\")\n",
    "            # Fallback if it fails\n",
    "            for clip in clips:\n",
    "                clip[\"visual_data\"] = {\"brightness\": \"unknown\", \"emotion\": \"unknown\"}\n",
    "            return clips\n",
    "\n",
    "        # Parse the JSON output from the script\n",
    "        # The script prints some logs, but the last line should be the JSON\n",
    "        lines = stdout.strip().split('\\n')\n",
    "        result_json_str = lines[-1] \n",
    "        results = json.loads(result_json_str)\n",
    "        \n",
    "        # Merge back\n",
    "        for clip in clips:\n",
    "            mid_point = (clip[\"start\"] + clip[\"end\"]) / 2\n",
    "            key = str(mid_point)\n",
    "            if key in results:\n",
    "                clip[\"visual_data\"] = results[key]\n",
    "            else:\n",
    "                clip[\"visual_data\"] = {\"brightness\": \"unknown\", \"emotion\": \"unknown\"}\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run isolated visual analyzer: {e}\")\n",
    "        for clip in clips:\n",
    "             clip[\"visual_data\"] = {\"brightness\": \"unknown\", \"emotion\": \"unknown\"}\n",
    "\n",
    "    return clips\n",
    "\n",
    "# --- NEW: THE BATCH PROCESSOR ---\n",
    "def process_batch_pipeline(video_paths_list, project_name=\"Project_01\", output_dir=\"uploads\", progress_callback=None, user_description=None, api_key=None):\n",
    "    \"\"\"\n",
    "    Takes a LIST of videos (e.g., ['intro.mp4', 'scene.mp4'])\n",
    "    and combines them into ONE Master JSON.\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\ude80 Starting Batch Process for {len(video_paths_list)} videos...\")\n",
    "    if progress_callback: progress_callback(0, \"Starting batch process...\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    master_timeline = []\n",
    "    global_id_counter = 1\n",
    "    total_videos = len(video_paths_list)\n",
    "    \n",
    "    for i, video_path in enumerate(video_paths_list):\n",
    "        base_progress = (i / total_videos) * 80\n",
    "        progress_per_video = 80 / total_videos\n",
    "        \n",
    "        print(f\"   ...Processing {os.path.basename(video_path)}\")\n",
    "        if progress_callback: progress_callback(base_progress + 5, f\"Extracting Audio: {os.path.basename(video_path)}\")\n",
    "        \n",
    "        # 1. Run our standard analysis\n",
    "        print(f\"      [1/3] Extracting Audio for {video_path}...\")\n",
    "        audio_path = extract_audio(video_path)\n",
    "        \n",
    "        if progress_callback: progress_callback(base_progress + (progress_per_video * 0.3), f\"Transcribing: {os.path.basename(video_path)}\")\n",
    "        print(f\"      [2/3] Transcribing Audio for {video_path}...\")\n",
    "        clips = transcribe_audio(audio_path)\n",
    "        \n",
    "        if progress_callback: progress_callback(base_progress + (progress_per_video * 0.6), f\"Visual Analysis: {os.path.basename(video_path)}\")\n",
    "        print(f\"      [3/3] Analyzing Visuals for {video_path}...\")\n",
    "        clips = analyze_visuals(video_path, clips)\n",
    "        \n",
    "        # 2. Tag them with the Source File (CRITICAL for editing later)\n",
    "        for clip in clips:\n",
    "            clip[\"id\"] = global_id_counter  # Unique ID across ALL videos\n",
    "            clip[\"source_video\"] = os.path.basename(video_path) # Remember where it came from\n",
    "            master_timeline.append(clip)\n",
    "            global_id_counter += 1\n",
    "            \n",
    "    # 3. Save the Master JSON\n",
    "    if progress_callback: progress_callback(85, \"Saving analysis data...\")\n",
    "    project_data = {\n",
    "        \"project_name\": project_name,\n",
    "        \"total_clips\": len(master_timeline),\n",
    "        \"timeline\": master_timeline\n",
    "    }\n",
    "    \n",
    "    output_json_path = os.path.join(output_dir, f\"{project_name}_analysis.json\")\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(project_data, f, indent=4)\n",
    "        \n",
    "    print(f\"\u2705 BATCH COMPLETE! Master JSON saved to: {output_json_path}\")\n",
    "    \n",
    "    # 4. Generate XML EDL for Frontend\n",
    "    if progress_callback: progress_callback(90, \"AI Generating Timeline (this may take a moment)...\")\n",
    "    output_xml_path = os.path.join(output_dir, f\"{project_name}.xml\")\n",
    "    \n",
    "    generate_xml_edl(project_data, output_xml_path, project_name, user_description, api_key=api_key)\n",
    "    if progress_callback: progress_callback(100, \"Done!\")\n",
    "    \n",
    "    print(f\"\u2705 XML EDL saved to: {output_xml_path}\")\n",
    "    \n",
    "    return output_json_path\n",
    "\n",
    "def generate_xml_edl(project_data, output_path, project_name=\"Project\", user_description=None, api_key=None):\n",
    "    print(\"\ud83e\udde0 Asking Llama 3 to edit the video...\")\n",
    "    \n",
    "    # User instructions injection\n",
    "    user_context = \"\"\n",
    "    if user_description:\n",
    "        user_context = f\"\"\"\n",
    "        USER INSTRUCTIONS:\n",
    "        The user has provided the following description/context for this edit.\n",
    "        You MUST prioritize these instructions when selecting clips, style, and tone:\n",
    "        \"{user_description}\"\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        from . import llm_config\n",
    "        key = api_key if api_key else llm_config.GEMINI_API_KEY\n",
    "        \n",
    "        # Configure Gemini (New SDK)\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=key)\n",
    "        \n",
    "        # Convert JSON to string\n",
    "        json_input = json.dumps(project_data, indent=2)\n",
    "        \n",
    "        # Use user_description if provided, otherwise a default\n",
    "        user_desc = user_description if user_description else 'Make it viral and fast-paced.'\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        ROLE: Expert Video Editor, Linguist, and Colorist.\n",
    "        \n",
    "        INPUT DATA (Sanitized but may still contain errors):\n",
    "        {json_input}\n",
    "        \n",
    "        USER CONTEXT: \"{user_desc}\"\n",
    "        \n",
    "        ---------------------------------------------------------\n",
    "        YOUR 5-STEP MISSION (THE \"WAKULLAH\" PROTOCOL):\n",
    "        ---------------------------------------------------------\n",
    "        \n",
    "        STEP 1: TEXT SANITIZATION (The Ghostbuster Filter)\n",
    "        - The transcript may still contain phantom words (e.g., \"Banana\", \"Penguin\", \"Steam\").\n",
    "        - RULE: If a word is a random noun that doesn't fit the sentence context, DELETE IT.\n",
    "        - RULE: Fix phonetic errors (e.g., \"Pre-ill\" -> \"Premiere\", \"Strain moral\" -> \"Train models\").\n",
    "        - OUTPUT: Use this CLEANED text in the final XML.\n",
    "        \n",
    "        STEP 2: SURGICAL EDITING (Bad Takes & Quality Control)\n",
    "        - Look for semantic duplicates (e.g., \"The first step... [pause]... The first step is...\").\n",
    "        - ACTION: Keep ONLY the best/last version. Mark the others as keep=\"false\".\n",
    "        - RULE: Cut \"dead air\" by adjusting 'start' and 'end' times to match the clean speech.\n",
    "        - CRITICAL RULE (QUALITY CONTROL):\n",
    "          - If a clip contains broken grammar, stuttering that breaks flow, or nonsense words, SET keep=\"false\" reason=\"Bad Grammar/Flow\".\n",
    "          - If a clip is just laughing, breathing, coughing, or silence with no meaningful speech, SET keep=\"false\" reason=\"Non-verbal/Noise\".\n",
    "          - If the transcript is unintelligible or hallucinated (random words), SET keep=\"false\" reason=\"Bad Audio/Transcript\".\n",
    "        \n",
    "        STEP 3: VISUAL REPAIR (The \"Fix It\" Logic)\n",
    "        - Check 'visual_data' for each clip.\n",
    "        - IF brightness is \"dark\" or \"low\":\n",
    "          - DO NOT DELETE. Instead, ADD: <correction type=\"brightness\" value=\"1.4\" />\n",
    "        - IF emotion is \"dull\":\n",
    "          - ADD: <correction type=\"saturation\" value=\"1.2\" />\n",
    "          \n",
    "        STEP 4: VIRAL ENHANCEMENTS (Overlays)\n",
    "        - Identify 3-5 \"High Value\" moments (Topic shifts, Punchlines).\n",
    "        - GENERATE <overlays> for them.\n",
    "        - Style: \"pop\", \"slide_up\" | \"typewriter\".\n",
    "        - Colors: Yellow (#FFFF00) for emphasis, White (#FFFFFF) for standard.\n",
    "        \n",
    "        STEP 5: VIRAL SHORTS (The Hook)\n",
    "        - Identify 2 separate sequences (15s-60s) that act as standalone viral shorts.\n",
    "        - Add them to the <viral_shorts> section.\n",
    "        \n",
    "        ---------------------------------------------------------\n",
    "        OUTPUT FORMAT (Strict XML):\n",
    "        ---------------------------------------------------------\n",
    "        <project name=\"{project_name}\">\n",
    "            <global_settings>\n",
    "                <frame_rate>30</frame_rate>\n",
    "            </global_settings>\n",
    "            \n",
    "            <edl>\n",
    "                <clip id=\"1\" source=\"video.mp4\" start=\"0.5\" end=\"4.2\" keep=\"true\" reason=\"Clean intro\" text=\"Welcome to the AI editor\">\n",
    "                    <correction type=\"brightness\" value=\"1.3\" /> \n",
    "                </clip>\n",
    "                \n",
    "                <clip id=\"2\" source=\"video.mp4\" start=\"4.2\" end=\"8.0\" keep=\"false\" reason=\"Redundant / Bad Audio\" />\n",
    "            </edl>\n",
    "            \n",
    "            <viral_shorts>\n",
    "                <short>\n",
    "                    <title>The Secret Trick</title>\n",
    "                    <clip_ids>5,6,7</clip_ids>\n",
    "                </short>\n",
    "            </viral_shorts>\n",
    "            \n",
    "            <overlays>\n",
    "                <text id=\"t1\" content=\"GAME CHANGER\" start=\"0.5\" duration=\"2.0\" style=\"pop\" color=\"#FFFF00\" size=\"5\" x=\"50\" y=\"50\" font=\"Arial-Bold\"/>\n",
    "            </overlays>\n",
    "        </project>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call Gemini (Standard SDK)\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean Output\n",
    "        # Handle potential safety block or empty response\n",
    "        if not response.text:\n",
    "             print(\"AI returned empty response (Safety Block?)\")\n",
    "             raise ValueError(\"AI Safety Block\")\n",
    "\n",
    "        xml_out = response.text.replace(\"```xml\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(xml_out)\n",
    "            \n",
    "        print(f\"\u2728 Master EDL Generated with Wakullah Protocol!\")\n",
    "        return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c AI Generation Failed: {e}\")\n",
    "        # Fallback dump for manual review\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(f\"<project name='{project_name}'><error>AI Failed: {str(e)}</error></project>\")\n",
    "\n",
    "    # 3. Fallback Manual Logic (if LLM fails)\n",
    "    print(\"\u2699\ufe0f Running manual fallback logic...\")\n",
    "    edl_content = f'<project name=\"{project_name}\">\\n'\n",
    "    edl_content += '  <global_settings>\\n    <filter_suggestion>Natural Grade</filter_suggestion>\\n'\n",
    "    edl_content += '    <color_grading>\\n      <temperature>5600</temperature>\\n      <exposure>0</exposure>\\n      <contrast>0</contrast>\\n      <saturation>100</saturation>\\n      <filter_strength>100</filter_strength>\\n    </color_grading>\\n  </global_settings>\\n'\n",
    "    edl_content += '  <edl>\\n'\n",
    "    \n",
    "    for clip in project_data.get(\"timeline\", []):\n",
    "        keep = \"true\"\n",
    "        reason = \"Manual Fallback\"\n",
    "        visuals = clip.get(\"visual_data\", {})\n",
    "        \n",
    "        if visuals.get(\"brightness\") == \"dark\":\n",
    "             keep = \"false\"\n",
    "             reason = \"Too dark (Manual)\"\n",
    "             \n",
    "        escaped_text = clip.get(\"text\", \"\").replace('\"', \"'\")\n",
    "        edl_content += f'    <clip id=\"{clip.get(\"id\")}\" source=\"{clip.get(\"source_video\")}\" start=\"{clip.get(\"start\")}\" end=\"{clip.get(\"end\")}\" keep=\"{keep}\" reason=\"{reason}\" text=\"{escaped_text}\" duration=\"{clip.get(\"end\") - clip.get(\"start\")}\">\\n'\n",
    "        edl_content += '      <color_grading>\\n        <temperature>5600</temperature>\\n        <exposure>0</exposure>\\n        <contrast>0</contrast>\\n        <saturation>100</saturation>\\n        <filter_strength>100</filter_strength>\\n      </color_grading>\\n'\n",
    "        edl_content += '    </clip>\\n'\n",
    "        \n",
    "    edl_content += '  </edl>\\n  <viral_shorts>\\n  </viral_shorts>\\n  <overlays>\\n  </overlays>\\n</project>'\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(edl_content)\n",
    "        \n",
    "    return False\n",
    "\n",
    "# --- TEST AREA ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with a list of videos\n",
    "    # Make sure you have these files or change the names!\n",
    "    test_files = [\"uploads/WTN1.mp4\",\"uploads/WTN2.mp4\",\"uploads/WTN3.mp4\"] \n",
    "    \n",
    "    # You can add more: test_files = [\"uploads/intro.mp4\", \"uploads/main.mp4\"]\n",
    "    \n",
    "    if os.path.exists(test_files[0]):\n",
    "        process_batch_pipeline(test_files, \"Test_Project\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 logic Renderer (`backend/renderer.py`)\n",
    "This module uses MoviePy to physically cut, grade, and stitch the video based on the AI's decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy import VideoFileClip, concatenate_videoclips, vfx, AudioFileClip, CompositeAudioClip, TextClip, CompositeVideoClip\n",
    "try:\n",
    "    from moviepy.audio.fx.all import audio_loop\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try finding it in other locations\n",
    "        from moviepy.audio.fx.audio_loop import audio_loop\n",
    "    except ImportError:\n",
    "        # Manual implementation if missing\n",
    "        from moviepy.audio.AudioClip import concatenate_audioclips\n",
    "        def audio_loop(audioclip, duration=None, n=None):\n",
    "            if duration is not None:\n",
    "                n = int(duration / audioclip.duration) + 1\n",
    "            elif n is None:\n",
    "                n = 1\n",
    "            \n",
    "            # Create a list of copies\n",
    "            clips = [audioclip] * n\n",
    "            # Concatenate\n",
    "            new_clip = concatenate_audioclips(clips)\n",
    "            \n",
    "            if duration is not None:\n",
    "                new_clip = new_clip.subclipped(0, duration)\n",
    "            return new_clip\n",
    "\n",
    "# Ensure concatenate_audioclips is available if we used it above, but also for general use\n",
    "from moviepy.audio.AudioClip import concatenate_audioclips\n",
    "\n",
    "\n",
    "from proglog import ProgressBarLogger\n",
    "EXPORT_DIR = \"exports\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "class RenderLogger(ProgressBarLogger):\n",
    "    def __init__(self, callback=None):\n",
    "        super().__init__()\n",
    "        self.prog_notifier = callback\n",
    "        self.last_message = \"\"\n",
    "\n",
    "    def callback_message(self, message):\n",
    "        self.last_message = message\n",
    "        if self.prog_notifier:\n",
    "            self.prog_notifier({\"status\": \"rendering\", \"message\": message})\n",
    "\n",
    "    def bars_callback(self, bar, attr, value, old_value=None):\n",
    "        if self.prog_notifier and \"total\" in self.bars[bar]:\n",
    "            total = self.bars[bar][\"total\"]\n",
    "            if total > 0:\n",
    "                percentage = (value / total) * 100\n",
    "                self.prog_notifier({\"status\": \"rendering\", \"progress\": percentage, \"message\": self.last_message})\n",
    "\n",
    "def create_motion_text(content, duration=2.0, style='pop', fontsize=70, color='white', font='Arial-Bold', pos=None, resize_func=None, max_width=None):\n",
    "    \"\"\"\n",
    "    Creates a TextClip using a double-layer technique for clean strokes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        effective_fontsize = int(fontsize)\n",
    "        # Moderate stroke width - not too thick to prevent text balloon effect\n",
    "        # 8% provides good visibility without making text massive\n",
    "        stroke_w = max(4, int(effective_fontsize * 0.08))\n",
    " \n",
    "        \n",
    "        # Method 'caption' allows wrapping. 'label' does not.\n",
    "        # If max_width is provided, use caption.\n",
    "        method = 'caption' if max_width else 'label'\n",
    "        \n",
    "        # 1. Stroke Layer (Background)\n",
    "        # We start with a base config\n",
    "        def make_clip(c_color, c_stroke_color, c_stroke_width):\n",
    "            # Base arguments for TextClip\n",
    "            text_args = {\n",
    "                'text': content,\n",
    "                'font_size': effective_fontsize,\n",
    "                'color': c_color,\n",
    "                'stroke_color': c_stroke_color,\n",
    "                'stroke_width': c_stroke_width,\n",
    "                'method': method\n",
    "            }\n",
    "            \n",
    "            # Only add size if max_width is provided\n",
    "            if max_width:\n",
    "                text_args['size'] = (max_width, None)\n",
    "            \n",
    "            try:\n",
    "                # Try with specified font\n",
    "                return TextClip(**text_args, font=font)\n",
    "            except:\n",
    "                # Fallback to 'Arial'\n",
    "                try:\n",
    "                    return TextClip(**text_args, font='Arial')\n",
    "                except:\n",
    "                    # Final fallback to default font\n",
    "                    text_args_minimal = {k: v for k, v in text_args.items() if k != 'font'}\n",
    "                    return TextClip(**text_args_minimal)\n",
    "\n",
    "        # Create the Stroke Layer (Black text with thick black stroke)\n",
    "        # Note: If we just use stroke_width on the main clip, it renders INSIDE.\n",
    "        # By compositing, we get the 'Outside' stroke effect.\n",
    "        \n",
    "        # 1. Use passed font size (already calculated with boost in main loop)\n",
    "        calc_fontsize = int(fontsize)\n",
    "\n",
    "        # 2. Layer Generation Helper\n",
    "        def make_clip(c_color, c_stroke_color, c_stroke_width):\n",
    "            text_args = {\n",
    "                'text': content,\n",
    "                'font_size': calc_fontsize,\n",
    "                'color': c_color,\n",
    "                'stroke_color': c_stroke_color,\n",
    "                'stroke_width': c_stroke_width,\n",
    "                'method': method\n",
    "            }\n",
    "            if max_width: text_args['size'] = (max_width, None)\n",
    "            \n",
    "            # Correctly use function argument 'font'\n",
    "            try: return TextClip(**text_args, font=font)\n",
    "            except: \n",
    "                try: return TextClip(**text_args, font='Arial')\n",
    "                except: return TextClip(**{k: v for k, v in text_args.items() if k != 'font'})\n",
    "\n",
    "        # 3. Create Layers\n",
    "        # A. Shadow Layer (Offset, slightly blurry/transparent look via color)\n",
    "        txt_shadow = make_clip('black', None, 0)\n",
    "        \n",
    "        # B. Stroke Layer (Background Outline)\n",
    "        txt_stroke = make_clip('black', 'black', stroke_w)\n",
    "        \n",
    "        # C. Fill Layer (Foreground)\n",
    "        # Correctly use function argument 'color'\n",
    "        txt_fill = make_clip(color, None, 0)\n",
    "\n",
    "        # 4. Fade Effects\n",
    "        if style == 'fade':\n",
    "            # Apply to all layers\n",
    "            for clip_layer in [txt_shadow, txt_stroke, txt_fill]:\n",
    "                try: clip_layer.fadein(0.5).fadeout(0.5)\n",
    "                except: pass\n",
    "\n",
    "        # 5. Composite & Align\n",
    "        # We need a canvas big enough for the stroke + shadow\n",
    "        # Calculations:\n",
    "        # stroke layer is biggest normally.\n",
    "        # shadow is offset by pixels.\n",
    "        \n",
    "        shadow_off = max(4, int(calc_fontsize * 0.05)) # 5% of font size as shadow offset\n",
    "        padding = stroke_w + shadow_off + 10 # ample padding\n",
    "        \n",
    "        # Dimensions are based on the stroke layer (largest)\n",
    "        base_w, base_h = txt_stroke.size\n",
    "        comp_w = base_w + (padding * 2)\n",
    "        comp_h = base_h + (padding * 2)\n",
    "        \n",
    "        # Center the Stroke Layer in the padded composite\n",
    "        stroke_pos = (padding, padding)\n",
    "        \n",
    "        # Center the Fill Layer RELATIVE to the Stroke Layer\n",
    "        # (Fill is smaller than stroke, so we center it to avoid 'glitchy' offset)\n",
    "        fill_dx = (txt_stroke.size[0] - txt_fill.size[0]) / 2\n",
    "        fill_dy = (txt_stroke.size[1] - txt_fill.size[1]) / 2\n",
    "        fill_pos = (padding + fill_dx, padding + fill_dy)\n",
    "        \n",
    "        # Shadow is same as Fill but offset\n",
    "        shadow_pos = (fill_pos[0] + shadow_off, fill_pos[1] + shadow_off)\n",
    "        \n",
    "        # Position the clips\n",
    "        ts = txt_stroke.with_position(stroke_pos)\n",
    "        tf = txt_fill.with_position(fill_pos)\n",
    "        tshadow = txt_shadow.with_position(shadow_pos)\n",
    "        \n",
    "        # Order: Shadow -> Stroke -> Fill\n",
    "        txt = CompositeVideoClip(\n",
    "            [tshadow, ts, tf], \n",
    "            size=(comp_w, comp_h)\n",
    "        )\n",
    "            \n",
    "        txt = txt.with_duration(duration)\n",
    "        \n",
    "        # Positioning Logic - Only apply style-based positioning if no custom pos provided\n",
    "        # Custom positioning will be applied AFTER creation by the caller\n",
    "        if pos is None:\n",
    "            # Apply style-based defaults only when no custom position\n",
    "            if style == 'slide_up':\n",
    "                txt = txt.with_position(('center', 0.8), relative=True) \n",
    "            elif style == 'fade':\n",
    "                txt = txt.with_position('center')\n",
    "                # Fade effects already applied above\n",
    "            elif style == 'typewriter':\n",
    "                txt = txt.with_position(('left', 'bottom'))\n",
    "            else:\n",
    "                txt = txt.with_position('center')\n",
    "        # If pos is provided, don't set position here - let caller handle it\n",
    "            \n",
    "        return txt\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error creating TextClip (Final): {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def apply_grading(clip, grading_settings):\n",
    "    \"\"\"\n",
    "    Applies color grading using raw NumPy manipulation for guaranteed results.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Parse Settings\n",
    "    try:\n",
    "        temp = float(grading_settings.get('temperature', 5600))\n",
    "        exp = float(grading_settings.get('exposure', 0.0))\n",
    "        con = float(grading_settings.get('contrast', 0))\n",
    "        sat = float(grading_settings.get('saturation', 100))\n",
    "        filter_name = grading_settings.get('filterSuggestion', 'None')\n",
    "    except:\n",
    "        return clip\n",
    "\n",
    "    # Check if we need to do anything (optimization)\n",
    "    if temp == 5600 and exp == 0 and con == 0 and sat == 100 and filter_name == 'None':\n",
    "        return clip\n",
    "\n",
    "    print(f\"\ud83c\udfa8 NumPy Grading -> T:{temp} E:{exp} C:{con} S:{sat} F:{filter_name}\", flush=True)\n",
    "\n",
    "    def filter_frame(image):\n",
    "        # Image is a numpy array [H, W, 3] uint8\n",
    "        img = image.astype(float)\n",
    "\n",
    "        # 1. Temperature (Simplified)\n",
    "        if temp != 5600:\n",
    "            val = (temp - 5600) / 5000.0 # -0.8 to +0.8\n",
    "            r_gain = 1 + (val * 0.2)\n",
    "            b_gain = 1 - (val * 0.2)\n",
    "            img[:, :, 0] *= r_gain\n",
    "            img[:, :, 2] *= b_gain\n",
    "\n",
    "        # 2. Exposure\n",
    "        if exp != 0:\n",
    "            factor = 2 ** exp\n",
    "            img *= factor\n",
    "\n",
    "        # 3. Contrast\n",
    "        if con != 0:\n",
    "            factor = 1 + (con / 100.0)\n",
    "            # Pivot around 128 (midpoint)\n",
    "            img = (img - 128.0) * factor + 128.0\n",
    "\n",
    "        # 4. Saturation (Simple RGB separation)\n",
    "        if sat != 100:\n",
    "             factor = sat / 100.0\n",
    "             # Gray is approx magnitude of RGB\n",
    "             # simple avg for speed \n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray + (img - gray) * factor\n",
    "\n",
    "        # 5. Simple Filter Presets (Overwrites)\n",
    "        # 5. Advanced Filter Presets\n",
    "        if filter_name == \"Cinematic\":\n",
    "             # High contrast, slight desaturation, moody\n",
    "             img = (img - 128.0) * 1.2 + 128.0\n",
    "             img *= 0.95\n",
    "        \n",
    "        elif filter_name == \"Teal & Orange\":\n",
    "             # Shadow -> Teal, Highlight -> Orange\n",
    "             # Simplified Channel Mixer\n",
    "             r = img[:, :, 0]\n",
    "             g = img[:, :, 1]\n",
    "             b = img[:, :, 2]\n",
    "             \n",
    "             # Boost Red in highlights (Orange)\n",
    "             r = np.where(r > 128, r * 1.2, r * 0.9)\n",
    "             # Boost Blue in shadows (Teal)\n",
    "             b = np.where(b < 128, b * 1.2, b * 0.9)\n",
    "             \n",
    "             img[:, :, 0] = np.clip(r, 0, 255)\n",
    "             img[:, :, 2] = np.clip(b, 0, 255)\n",
    "             \n",
    "             # Contrast bump\n",
    "             img = (img - 128.0) * 1.1 + 128.0\n",
    "\n",
    "        elif filter_name == \"Vintage\":\n",
    "             # Sepia-ish\n",
    "             # R * 1.1, B * 0.9 + Lift Blacks\n",
    "             img[:, :, 0] *= 1.1 # R\n",
    "             img[:, :, 2] *= 0.85 # B\n",
    "             img = (img - 128.0) * 0.9 + 128.0 # Lower contrast\n",
    "             img += 10 # Lift blacks\n",
    "\n",
    "        elif filter_name == \"Noir\":\n",
    "             # B&W + High Contrast\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray\n",
    "             img = (img - 128.0) * 1.5 + 128.0\n",
    "        \n",
    "        elif filter_name == \"Vivid\":\n",
    "             # Boost sat strongly\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray + (img - gray) * 1.5\n",
    "        \n",
    "        elif filter_name == \"Vivid Warm\":\n",
    "             # Boost sat + Warm temp\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray + (img - gray) * 1.3\n",
    "             img[:, :, 0] *= 1.1 # R up\n",
    "             img[:, :, 2] *= 0.9 # B down\n",
    "\n",
    "        elif filter_name == \"Vivid Cool\":\n",
    "             # Boost sat + Cool temp\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray + (img - gray) * 1.3\n",
    "             img[:, :, 0] *= 0.9 # R down\n",
    "             img[:, :, 2] *= 1.1 # B up\n",
    "\n",
    "        elif filter_name == \"Dramatic\":\n",
    "             # Desaturated + High Contrast\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray + (img - gray) * 0.8 # Desaturate\n",
    "             img = (img - 128.0) * 1.4 + 128.0 # High Contrast\n",
    "\n",
    "        elif filter_name == \"Mono\" or filter_name == \"B&W\":\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray \n",
    "             \n",
    "        elif filter_name == \"Silvertone\":\n",
    "             # B&W + Brightness push + Contrast\n",
    "             gray = np.mean(img, axis=2, keepdims=True)\n",
    "             img = gray\n",
    "             img = (img - 128.0) * 1.2 + 128.0\n",
    "             img *= 1.1 # Bright\n",
    "\n",
    "        # Clip and cast back\n",
    "        return np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Use fl_image if available (standard in MoviePy)\n",
    "    # If using MoviePy 2.x, it might be renamed, but fl_image usually persists.\n",
    "    if hasattr(clip, 'fl_image'):\n",
    "        return clip.fl_image(filter_frame)\n",
    "    else:\n",
    "        # Fallback for very new versions if fl_image is gone\n",
    "        return clip.transform(lambda get_frame, t: filter_frame(get_frame(t)))\n",
    "\n",
    "import time\n",
    "\n",
    "import traceback\n",
    "\n",
    "def render_project(project_data, progress_callback=None):\n",
    "    \"\"\"\n",
    "    1. Reads the instructions from React\n",
    "    2. Cuts the video\n",
    "    3. Stitches it together\n",
    "    4. Exports to MP4\n",
    "    \"\"\"\n",
    "    print(\"\ud83c\udfac Starting Render Job...\")\n",
    "    if progress_callback:\n",
    "        progress_callback({\"status\": \"processing\", \"progress\": 0, \"message\": \"Starting Render Job...\"})\n",
    "\n",
    "    final_clips = []\n",
    "    \n",
    "    # We might need to adjust paths if running from root\n",
    "    # \"uploads\" folder is likely in root.\n",
    "    UPLOAD_DIR = \"uploads\"\n",
    "    \n",
    "    try:\n",
    "        # Extract Global Settings\n",
    "        # Handle cases where keys might be missing or different casing\n",
    "        global_settings = project_data.get('globalSettings', {})\n",
    "        global_grading = global_settings.get('colorGrading', {})\n",
    "        global_filter = global_settings.get('filterSuggestion', 'None')\n",
    "        \n",
    "        # Ensure we have a dict\n",
    "        if not isinstance(global_grading, dict): global_grading = {}\n",
    "        \n",
    "        # Determine clips list (handle 'edl' or 'clips' key)\n",
    "        clips_list = project_data.get('edl', project_data.get('clips', []))\n",
    "        total_clips = len(clips_list)\n",
    "        processed_count = 0\n",
    "\n",
    "        for i, clip_data in enumerate(clips_list):\n",
    "            # SKIP clips the user marked as 'Red/Remove'\n",
    "            # Handle boolean or string \"false\"\n",
    "            keep_val = clip_data.get('keep', True)\n",
    "            if isinstance(keep_val, str) and keep_val.lower() == 'false':\n",
    "                 keep_val = False\n",
    "            \n",
    "            if not keep_val:\n",
    "                print(f\"\u2702\ufe0f Skipping clip {clip_data.get('id')} (keep={keep_val})\")\n",
    "                continue\n",
    "                \n",
    "            source_name = clip_data['source']\n",
    "            if progress_callback:\n",
    "                progress_callback({\n",
    "                    \"status\": \"processing\", \n",
    "                    \"progress\": (i / total_clips) * 10,  # First 10% is for loading/clipping\n",
    "                    \"message\": f\"Processing clip {i+1}/{total_clips}\"\n",
    "                })\n",
    "            \n",
    "            try:\n",
    "                # A. Resolve Source Path\n",
    "                # Priority 1: Uploads folder (Legacy)\n",
    "                source_path = os.path.join(UPLOAD_DIR, os.path.basename(source_name))\n",
    "                \n",
    "                # Priority 2: Project Specific Folder\n",
    "                if not os.path.exists(source_path):\n",
    "                     p_name = project_data.get('name', '')\n",
    "                     # Try exact name\n",
    "                     safe_name = \"\".join(c for c in p_name if c.isalnum() or c in (' ', '_', '-')).strip()\n",
    "                     project_media_path = os.path.join(\"projects\", safe_name, \"source_media\", os.path.basename(source_name))\n",
    "                     \n",
    "                     if os.path.exists(project_media_path):\n",
    "                         source_path = project_media_path\n",
    "                     else:\n",
    "                         # Try heuristic for Shorts (e.g. \"LateShow_Short1\" -> \"LateShow\")\n",
    "                         parts = safe_name.split('_')\n",
    "                         if len(parts) > 1:\n",
    "                             base_name = parts[0]\n",
    "                             heuristic_path = os.path.join(\"projects\", base_name, \"source_media\", os.path.basename(source_name))\n",
    "                             if os.path.exists(heuristic_path):\n",
    "                                 source_path = heuristic_path\n",
    "                \n",
    "                # Priority 3: Absolute Fallback (for testing)\n",
    "                if not os.path.exists(source_path):\n",
    "                    abs_fallback = os.path.join(\"/Users/saieshwarrampelli/Downloads/GravityEdits/source_media\", os.path.basename(source_name))\n",
    "                    if os.path.exists(abs_fallback):\n",
    "                        source_path = abs_fallback\n",
    "                    \n",
    "                    # Priority 4: Search blindly in projects dir? (Too slow/risky)\n",
    "                    \n",
    "                    # Heuristic: If source_name has no extension, try adding .mp4 or .mov\n",
    "                    if not os.path.exists(source_path) and '.' not in source_name:\n",
    "                        for ext in ['.mp4', '.mov', '.mkv']:\n",
    "                            test_path = source_path + ext\n",
    "                            if os.path.exists(test_path):\n",
    "                                source_path = test_path\n",
    "                                break\n",
    "\n",
    "                if not os.path.exists(source_path):\n",
    "                     print(f\"\u26a0\ufe0f Source file missing: {source_path}\")\n",
    "                     continue\n",
    "\n",
    "                # A. Load Video\n",
    "                original_video = VideoFileClip(source_path)\n",
    "                \n",
    "                # B. Trim (The \"Scissors\" Logic)\n",
    "                # We use the start/end times we saved earlier\n",
    "                start = float(clip_data.get('start', 0))\n",
    "                end_val = clip_data.get('end')\n",
    "                \n",
    "                # If end is 0 or missing, use duration or video end\n",
    "                if not end_val or float(end_val) == 0:\n",
    "                     duration = float(clip_data.get('duration', 0))\n",
    "                     if duration > 0:\n",
    "                         end_val = start + duration\n",
    "                     else:\n",
    "                         end_val = original_video.duration\n",
    "\n",
    "                end = float(end_val)\n",
    "                \n",
    "                # Safety check: ensure we don't cut past the end of video\n",
    "                if end > original_video.duration: end = original_video.duration\n",
    "                if start >= end:\n",
    "                     print(f\"\u26a0\ufe0f Invalid trim for {clip_data['id']}: start {start} >= end {end}\")\n",
    "                     # Try to fix if it's just a small drift, otherwise skip\n",
    "                     if start < original_video.duration:\n",
    "                         end = original_video.duration\n",
    "                     else:\n",
    "                         continue\n",
    "                \n",
    "                cut_clip = original_video.subclipped(start, end)\n",
    "                \n",
    "                # C. Apply Color Filter / Grading\n",
    "                # Merge Global + Clip Grading\n",
    "                clip_grading = clip_data.get('colorGrading', {})\n",
    "                if not isinstance(clip_grading, dict): clip_grading = {}\n",
    "                \n",
    "                # Start with global\n",
    "                merged_settings = global_grading.copy()\n",
    "                # Override with clip specific (if non-zero/default)\n",
    "                # Actually, usually users want clip grading to ADD to global or REPLACE?\n",
    "                # For simplicity, we'll let clip-specific values override global provided they exist.\n",
    "                # Or better: if clip has specific grading, use it.\n",
    "                if clip_grading:\n",
    "                    merged_settings.update(clip_grading)\n",
    "                \n",
    "                # Pass the named filter too\n",
    "                merged_settings['filterSuggestion'] = global_filter\n",
    "                \n",
    "                cut_clip = apply_grading(cut_clip, merged_settings)\n",
    "\n",
    "                # D. Aspect Ratio Transformation (Shorts/Portrait)\n",
    "                # Check for renderMode explicitly\n",
    "                render_mode = project_data.get('renderMode', 'landscape')\n",
    "                \n",
    "                # Heuristic: If name implies short and no mode set? \n",
    "                # Better to rely on frontend flag we will add.\n",
    "\n",
    "                if render_mode == 'portrait':\n",
    "                     w, h = cut_clip.size\n",
    "                     target_ratio = 9/16\n",
    "                     \n",
    "                     # 1. Center Crop\n",
    "                     # Calculate target width for current height to match 9:16\n",
    "                     new_w = int(h * target_ratio)\n",
    "                     \n",
    "                     if new_w < w:\n",
    "                         # Landscape or Square -> Crop width\n",
    "                         center_x = w / 2\n",
    "                         x1 = center_x - (new_w / 2)\n",
    "                         cut_clip = cut_clip.crop(x1=x1, width=new_w, height=h)\n",
    "                         \n",
    "                     # 2. Resize to 1080x1920 (Standard HD Shorts)\n",
    "                     # Check if resize is needed to avoid unnecessary processing\n",
    "                     if cut_clip.size != (1080, 1920):\n",
    "                        cut_clip = cut_clip.resized((1080, 1920))\n",
    "\n",
    "                final_clips.append(cut_clip)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Error processing clip {clip_data.get('id')}: {e}\")\n",
    "                print(traceback.format_exc()) # CRITICAL: See exactly why it failed\n",
    "\n",
    "        if not final_clips:\n",
    "            print(\"\u274c No valid clips to render.\")\n",
    "            if progress_callback:\n",
    "                progress_callback({\"status\": \"failed\", \"message\": \"No valid clips to render (Check logs for details)\"})\n",
    "            return None\n",
    "\n",
    "        # E. Stitch (Concatenate)\n",
    "        print(f\"\ud83d\udd28 Stitching {len(final_clips)} clips together...\")\n",
    "        if progress_callback:\n",
    "                progress_callback({\"status\": \"processing\", \"progress\": 10, \"message\": \"Stitching clips...\"})\n",
    "        \n",
    "        final_video = concatenate_videoclips(final_clips, method=\"compose\")\n",
    "\n",
    "        # --- NEW: Process Overlays ---\n",
    "        overlays = project_data.get('overlays', [])\n",
    "        if overlays:\n",
    "            print(f\"\u2728 Adding {len(overlays)} text overlays...\")\n",
    "            print(f\"\ud83d\udcd0 Video resolution: {final_video.size[0]}x{final_video.size[1]}\")\n",
    "            overlay_clips = [final_video] # Base layer\n",
    "            \n",
    "            # Get video dimensions for calculations\n",
    "            vw, vh = final_video.size\n",
    "            print(f\"\ud83c\udfac Processing overlays for {vw}x{vh} video\")\n",
    "            \n",
    "            for overlay in overlays:\n",
    "                try:\n",
    "                    content = overlay.get('content', '')\n",
    "                    start = float(overlay.get('start', 0))\n",
    "                    dur = float(overlay.get('duration', 2.0))\n",
    "                    style = overlay.get('style', 'pop')\n",
    "                    \n",
    "                    # New properties from Frontend\n",
    "                    # fontSize is in 'percentage of width' (cqw equivalent)\n",
    "                    f_size_pct = overlay.get('fontSize', 4) \n",
    "                    \n",
    "                    # Position is % 0-100\n",
    "                    p_x = overlay.get('positionX')\n",
    "                    p_y = overlay.get('positionY')\n",
    "                    t_color = overlay.get('textColor', 'white')\n",
    "                    font_fam = overlay.get('fontFamily', 'Arial-Bold')\n",
    "                    \n",
    "                    # ========================================\n",
    "                    # NORMALIZED COORDINATE SYSTEM (0.0 to 1.0)\n",
    "                    # ========================================\n",
    "                    # Frontend sends:\n",
    "                    #   - fontSize: 0.0-1.0 (% of video HEIGHT)\n",
    "                    #   - positionX: 0.0-1.0 (center of text, 0=left, 1=right)\n",
    "                    #   - positionY: 0.0-1.0 (center of text, 0=top, 1=bottom)\n",
    "                    \n",
    "                    # 1. Calculate Font Size (% of video HEIGHT)\n",
    "                    try:\n",
    "                        f_norm = float(f_size_pct) if f_size_pct else 0.05\n",
    "                    except: \n",
    "                        f_norm = 0.05  # Default 5% of height\n",
    "                    \n",
    "                    # OLD DATA MIGRATION: If value > 1, assume it's old 0-100 format\n",
    "                    if f_norm > 1.0:\n",
    "                        f_norm = f_norm / 100.0\n",
    "                    \n",
    "                    # De-normalize: multiply by video HEIGHT\n",
    "                    # VISUAL CORRECTION: Added 1.5x multiplier because \"5% height\" in \n",
    "                    # standard video rendering looks smaller than \"5vh\" in CSS due to pixel density/weight.\n",
    "                    # This ensures the \"Bold Social Media\" look.\n",
    "                    calc_fontsize = int(vh * f_norm * 1.5)\n",
    "                    \n",
    "                    # Minimum size for readability\n",
    "                    if calc_fontsize < 60: calc_fontsize = 60\n",
    "                    \n",
    "                    print(f\"  \ud83d\udcdd Overlay '{content}': fontSize={f_norm:.3f} (norm) * 2.0 -> {calc_fontsize}px, color={t_color}\")\n",
    "\n",
    "                    # 2. Parse Position (normalized 0.0 to 1.0)\n",
    "                    try:\n",
    "                        pos_x = float(p_x) if p_x is not None else 0.5\n",
    "                        pos_y = float(p_y) if p_y is not None else 0.8\n",
    "                    except:\n",
    "                        pos_x, pos_y = 0.5, 0.8  # Default center-bottom\n",
    "                    \n",
    "                    # OLD DATA MIGRATION: If values > 1, assume old 0-100 format\n",
    "                    if pos_x > 1.0:\n",
    "                        pos_x = pos_x / 100.0\n",
    "                    if pos_y > 1.0:\n",
    "                        pos_y = pos_y / 100.0\n",
    "                    \n",
    "                    print(f\"    \u21b3 Position: ({pos_x:.3f}, {pos_y:.3f}) normalized\")\n",
    "                    \n",
    "                    # 3. Text wrapping\n",
    "                    # Only wrap for LONG text. Short text (like \"Hey\") should be single line\n",
    "                    safe_text_width = None\n",
    "                    if len(content) > 20:  # Only wrap long text\n",
    "                        safe_text_width = int(vw * 0.85)\n",
    "                    \n",
    "                    # 3. Create Clip - DON'T pass custom position to function\n",
    "                    # Let it apply style-based defaults, we'll override with custom position after\n",
    "                    txt = create_motion_text(\n",
    "                        content, \n",
    "                        duration=dur, \n",
    "                        style=style, \n",
    "                        fontsize=calc_fontsize, \n",
    "                        color=t_color, \n",
    "                        font=font_fam,\n",
    "                        pos=None,  # Always None - we handle positioning below\n",
    "                        max_width=safe_text_width\n",
    "                    )\n",
    "                    \n",
    "                    if txt:\n",
    "                        txt = txt.with_start(start)\n",
    "                        \n",
    "                        # 4. DE-NORMALIZE Position: Convert 0-1 to actual pixels\n",
    "                        # The position represents where the CENTER of the text should be\n",
    "                        tw, th = txt.size\n",
    "                        \n",
    "                        # De-normalize: multiply by video dimensions\n",
    "                        center_x = pos_x * vw\n",
    "                        center_y = pos_y * vh\n",
    "                        \n",
    "                        # Calculate Top-Left coordinate (anchoring at CENTER)\n",
    "                        tl_x = center_x - (tw / 2)\n",
    "                        tl_y = center_y - (th / 2)\n",
    "                        \n",
    "                        # Clamp to video bounds\n",
    "                        tl_x_original = tl_x\n",
    "                        tl_y_original = tl_y\n",
    "                        tl_x = max(0, min(tl_x, vw - tw))\n",
    "                        tl_y = max(0, min(tl_y, vh - th))\n",
    "                        \n",
    "                        txt = txt.with_position((tl_x, tl_y))\n",
    "                        \n",
    "                        # Log with warning if position was clamped\n",
    "                        if tl_x != tl_x_original or tl_y != tl_y_original:\n",
    "                            print(f\"    \u26a0\ufe0f  Position clamped to fit {vw}x{vh} video\")\n",
    "                        print(f\"    \u21b3 Text size: {tw}x{th}, center: ({center_x:.0f}, {center_y:.0f}) -> top-left: ({tl_x:.0f}, {tl_y:.0f})\")\n",
    "                        \n",
    "                        overlay_clips.append(txt)\n",
    "                        print(f\"  \u2705 Added overlay at {start}s for {dur}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\u274c Failed to add overlay {overlay}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            if len(overlay_clips) > 1:\n",
    "                # CompositeVideoClip allows layering\n",
    "                final_video = CompositeVideoClip(overlay_clips)\n",
    "        # -----------------------------\n",
    "\n",
    "        # G. Apply Audio Mixing (Background Music & Secondary Tracks)\n",
    "        audio_layers = []\n",
    "        if final_video.audio:\n",
    "            audio_layers.append(final_video.audio)\n",
    "        \n",
    "        # 1. Background Score (Legacy & Migrated)\n",
    "        bg_music_config = project_data.get('bgMusic')\n",
    "        if bg_music_config and bg_music_config.get('source'):\n",
    "            try:\n",
    "                music_file = bg_music_config['source']\n",
    "                print(f\"\ud83c\udfb5 Processing background music: {music_file}\")\n",
    "                print(f\"   Config: start={bg_music_config.get('start', 0)}, volume={bg_music_config.get('volume', 0.5)}, duration={bg_music_config.get('duration', 'auto')}\")\n",
    "                \n",
    "                # Search paths\n",
    "                music_path = os.path.join(UPLOAD_DIR, music_file)\n",
    "                if not os.path.exists(music_path):\n",
    "                     p_name = project_data.get('name')\n",
    "                     if p_name:\n",
    "                         safe_name = \"\".join(c for c in p_name if c.isalnum() or c in (' ', '_', '-')).strip()\n",
    "                         music_path_alt = os.path.join(\"projects\", safe_name, \"source_media\", music_file)\n",
    "                         if os.path.exists(music_path_alt):\n",
    "                             music_path = music_path_alt\n",
    "                             print(f\"   Found at: {music_path}\")\n",
    "                \n",
    "                if os.path.exists(music_path):\n",
    "                    print(f\"   \u2705 Loading audio from: {music_path}\")\n",
    "                    bg_music = AudioFileClip(music_path)\n",
    "                    print(f\"   \u2705 Audio loaded: duration={bg_music.duration:.2f}s, fps={bg_music.fps}\")\n",
    "                    \n",
    "                    # Handle volume - Check trackVolumes first, then bgMusic.volume\n",
    "                    track_volumes = project_data.get('trackVolumes', {})\n",
    "                    if 'music' in track_volumes:\n",
    "                        vol = float(track_volumes['music'])\n",
    "                        print(f\"   \ud83d\udd0a Using trackVolumes.music: {vol}\")\n",
    "                    else:\n",
    "                        vol = float(bg_music_config.get('volume', 0.5))\n",
    "                        print(f\"   \ud83d\udd0a Using bgMusic.volume: {vol}\")\n",
    "                    \n",
    "                    bg_music = bg_music.multiply_volume(vol)\n",
    "                    \n",
    "                    # Handle Start Time & Duration\n",
    "                    # If this is the \"Legacy\" bgMusic track that is now draggable:\n",
    "                    bg_start = float(bg_music_config.get('start', 0))\n",
    "                    bg_user_duration = bg_music_config.get('duration')\n",
    "                    \n",
    "                    video_duration = final_video.duration\n",
    "\n",
    "                    # If duration provided (it was split/trimmed), use it\n",
    "                    if bg_user_duration:\n",
    "                        dur = float(bg_user_duration)\n",
    "                        # Trim source to this duration? Or Loop until this duration?\n",
    "                        # Usually \"duration\" in timeline means \"length of clip\".\n",
    "                        # If source is shorter, loop. If source is longer, cut.\n",
    "                        if bg_music.duration < dur:\n",
    "                             bg_music = audio_loop(bg_music, duration=dur)\n",
    "                        else:\n",
    "                             bg_music = bg_music.subclipped(0, dur)\n",
    "                    else:\n",
    "                        # Legacy Loop Mode: Fill entire video\n",
    "                        # Calculate remaining time from start\n",
    "                        remaining_dur = max(0, video_duration - bg_start)\n",
    "                        if bg_music.duration < remaining_dur:\n",
    "                            bg_music = audio_loop(bg_music, duration=remaining_dur)\n",
    "                        else:\n",
    "                            bg_music = bg_music.subclipped(0, remaining_dur)\n",
    "\n",
    "                    bg_music = bg_music.with_start(bg_start)\n",
    "                    \n",
    "                    audio_layers.append(bg_music)\n",
    "                    print(f\"   \u2705 Background music added to audio layers (start={bg_start}s, volume={vol})\")\n",
    "                else:\n",
    "                    print(f\"   \u274c Music file not found at: {music_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Failed to add background music: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        elif bg_music_config:\n",
    "            print(f\"\u26a0\ufe0f bgMusic config exists but no source: {bg_music_config}\")\n",
    "        else:\n",
    "            print(f\"\u2139\ufe0f  No background music configured\")\n",
    "\n",
    "        # 2. Secondary Audio Clips (A2 / SFX / Split Music)\n",
    "        secondary_clips = project_data.get('audioClips', [])\n",
    "        if secondary_clips:\n",
    "            print(f\"\ud83d\udd0a Adding {len(secondary_clips)} secondary audio clips...\")\n",
    "            for clip_data in secondary_clips:\n",
    "                try:\n",
    "                    sfx_file = clip_data.get('source')\n",
    "                    start_time = float(clip_data.get('start', 0))\n",
    "                    user_duration = clip_data.get('duration')\n",
    "                    \n",
    "                    # Check paths\n",
    "                    sfx_path = os.path.join(UPLOAD_DIR, sfx_file)\n",
    "                    if not os.path.exists(sfx_path):\n",
    "                         p_name = project_data.get('name')\n",
    "                         if p_name:\n",
    "                             safe_name = \"\".join(c for c in p_name if c.isalnum() or c in (' ', '_', '-')).strip()\n",
    "                             sfx_path_alt = os.path.join(\"projects\", safe_name, \"source_media\", sfx_file)\n",
    "                             if os.path.exists(sfx_path_alt):\n",
    "                                 sfx_path = sfx_path_alt\n",
    "                    \n",
    "                    if os.path.exists(sfx_path):\n",
    "                        sfx_clip = AudioFileClip(sfx_path)\n",
    "                        \n",
    "                        # Handle Duration (Cutting)\n",
    "                        # If frontend created a \"Part 2\" clip, it usually expects the Playback to resume from the split point.\n",
    "                        # Wait, my frontend logic: \"part2 = {start: ..., duration: ...}\".\n",
    "                        # BUT it didn't specify \"media_start\" (start point in source file)!\n",
    "                        # Standard EDL usually has `timeline_start` AND `media_start`.\n",
    "                        # Currently, my `AudioClip` model in `types.ts` DOES NOT HAVE `mediaStart` or `inPoint`.\n",
    "                        # It is effectively assuming every clip starts from 0:00 of the source file.\n",
    "                        # THIS IS A BUG FOR SPLIT CLIPS!\n",
    "                        \n",
    "                        # FIX LOGIC: When splitting Audio, we must track where in the source file we are.\n",
    "                        # BUT `types.ts` AudioClip interface:\n",
    "                        # export interface AudioClip { id, source, start, duration, track }\n",
    "                        # It lacks `offset` or `trimStart`.\n",
    "                        # Meaning separate clips will ALL restart the music from the beginning (0:00).\n",
    "                        \n",
    "                        # I cannot fix this in renderer alone if the data isn't there.\n",
    "                        # However, for now, let's assume standard behavior: Clip starts at 0.\n",
    "                        # I will add `subclip(0, duration)` to respect the cut length.\n",
    "                        # But Part 2 will restart the song. The user will notice this.\n",
    "                        \n",
    "                        target_dur = float(user_duration) if user_duration else sfx_clip.duration\n",
    "                        if target_dur < sfx_clip.duration:\n",
    "                             sfx_clip = sfx_clip.subclipped(0, target_dur)\n",
    "                        \n",
    "                        sfx_clip = sfx_clip.with_start(start_time)\n",
    "                        \n",
    "                        # Clip if goes beyond video?\n",
    "                        # if start_time + sfx_clip.duration > final_video.duration:\n",
    "                        #    sfx_clip = sfx_clip.subclip(0, final_video.duration - start_time)\n",
    "                    \n",
    "                        audio_layers.append(sfx_clip)\n",
    "                except Exception as e:\n",
    "                    print(f\"\u26a0\ufe0f Failed to add secondary clip {clip_data}: {e}\")\n",
    "\n",
    "        # H. Final Audio Mixing\n",
    "        print(f\"\ud83c\udf9a\ufe0f  Mixing {len(audio_layers)} audio layers...\")\n",
    "        if len(audio_layers) > 0:\n",
    "             print(f\"   Audio layers: {[type(layer).__name__ for layer in audio_layers]}\")\n",
    "             final_audio = CompositeAudioClip(audio_layers)\n",
    "             final_video = final_video.with_audio(final_audio)\n",
    "             print(f\"   \u2705 Final audio composed and attached to video\")\n",
    "        else:\n",
    "             print(f\"   \u26a0\ufe0f  No audio layers to mix - video will be silent!\")\n",
    "        \n",
    "        # I. Export to MP4\n",
    "        timestamp = int(time.time())\n",
    "        output_filename = f\"{project_data.get('name', 'video')}_final_{timestamp}.mp4\"\n",
    "        output_path = os.path.join(EXPORT_DIR, output_filename)\n",
    "        \n",
    "        print(f\"\ud83d\udcbe Saving to {output_path}...\")\n",
    "        \n",
    "        # Create user logger\n",
    "        # Note: MoviePy 'logger' argument expects either 'bar', None, or a proglog logger\n",
    "        logger = RenderLogger(progress_callback) if progress_callback else 'bar'\n",
    "\n",
    "        final_video.write_videofile(\n",
    "            output_path, \n",
    "            fps=24, \n",
    "            preset=\"ultrafast\",  # Use 'medium' for better quality, 'ultrafast' for testing\n",
    "            codec=\"libx264\",\n",
    "            audio_codec=\"aac\",\n",
    "            ffmpeg_params=['-pix_fmt', 'yuv420p'], # Good for compatibility\n",
    "            logger=logger\n",
    "        )\n",
    "        \n",
    "        print(f\"\u2705 Video Saved: {output_path}\")\n",
    "        if progress_callback:\n",
    "            progress_callback({\"status\": \"completed\", \"progress\": 100, \"message\": \"Render Complete\", \"url\": f\"/exports/{output_filename}\"})\n",
    "        \n",
    "        # --- NEW: Generate Subtitles ---\n",
    "        try:\n",
    "            from . import subtitle_generator\n",
    "            srt_filename = output_filename.replace('.mp4', '.srt')\n",
    "            srt_path = os.path.join(EXPORT_DIR, srt_filename)\n",
    "            subtitle_generator.generate_srt(project_data, srt_path)\n",
    "            print(f\"\ud83d\udcdd Subtitles saved to: {srt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Subtitle generation failed: {e}\")\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Clean up\n",
    "        for c in final_clips:\n",
    "            c.close()\n",
    "            \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Rendering error: {e}\")\n",
    "        if progress_callback:\n",
    "            progress_callback({\"status\": \"failed\", \"message\": str(e)})\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Main API Service (`backend/main.py`)\n",
    "The FastAPI entry point handling requests and job queues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime\n",
    "from rq.job import Job\n",
    "from rq.exceptions import NoSuchJobError\n",
    "from .redis_config import redis_conn, q_render, q_analysis\n",
    "\n",
    "# Import our modules\n",
    "try:\n",
    "    from . import ai_engine\n",
    "    from . import renderer\n",
    "    from . import chat_engine\n",
    "except ImportError:\n",
    "    import ai_engine\n",
    "    import renderer\n",
    "    import chat_engine\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configure CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Directory Setup\n",
    "UPLOAD_DIR = \"uploads\" # Legacy bucket\n",
    "PROJECTS_DIR = \"projects\"\n",
    "EXPORT_DIR = \"exports\"\n",
    "\n",
    "for d in [UPLOAD_DIR, PROJECTS_DIR, EXPORT_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Mounts\n",
    "app.mount(\"/uploads\", StaticFiles(directory=UPLOAD_DIR), name=\"uploads\")\n",
    "app.mount(\"/exports\", StaticFiles(directory=EXPORT_DIR), name=\"exports\")\n",
    "app.mount(\"/projects\", StaticFiles(directory=PROJECTS_DIR), name=\"projects\") # Expose project assets\n",
    "\n",
    "# Utils\n",
    "def get_video_duration(file_path):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        if not cap.isOpened(): return 0\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        cap.release()\n",
    "        return duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting duration for {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def get_project_path(project_name):\n",
    "    # Sanitize project name simple check\n",
    "    safe_name = \"\".join(c for c in project_name if c.isalnum() or c in (' ', '_', '-')).strip()\n",
    "    return os.path.join(PROJECTS_DIR, safe_name)\n",
    "\n",
    "# --- PROJECTS API ---\n",
    "\n",
    "class ProjectCreate(BaseModel):\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "\n",
    "@app.get(\"/api/projects\")\n",
    "def list_projects():\n",
    "    projects = []\n",
    "    if os.path.exists(PROJECTS_DIR):\n",
    "        for dirname in os.listdir(PROJECTS_DIR):\n",
    "            path = os.path.join(PROJECTS_DIR, dirname)\n",
    "            if os.path.isdir(path):\n",
    "                # Try to read manifest\n",
    "                manifest_path = os.path.join(path, \"project.json\")\n",
    "                meta = {}\n",
    "                if os.path.exists(manifest_path):\n",
    "                    try:\n",
    "                        with open(manifest_path, 'r') as f: meta = json.load(f)\n",
    "                    except: pass\n",
    "                \n",
    "                projects.append({\n",
    "                    \"name\": dirname,\n",
    "                    \"created_at\": meta.get(\"created_at\"),\n",
    "                    \"thumbnail\": meta.get(\"thumbnail\"), # Could be path to first video thumb\n",
    "                    \"clip_count\": len(os.listdir(os.path.join(path, \"source_media\"))) if os.path.exists(os.path.join(path, \"source_media\")) else 0\n",
    "                })\n",
    "    return projects\n",
    "\n",
    "@app.post(\"/api/projects\")\n",
    "def create_project(data: ProjectCreate):\n",
    "    print(f\"Creating project: {data.name}\")\n",
    "    path = get_project_path(data.name)\n",
    "    if os.path.exists(path):\n",
    "        raise HTTPException(status_code=400, detail=\"Project already exists\")\n",
    "    \n",
    "    os.makedirs(path)\n",
    "    os.makedirs(os.path.join(path, \"source_media\"))\n",
    "    os.makedirs(os.path.join(path, \"exports\"))\n",
    "    \n",
    "    meta = {\n",
    "        \"name\": data.name,\n",
    "        \"description\": data.description,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"status\": \"created\"\n",
    "    }\n",
    "    with open(os.path.join(path, \"project.json\"), \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "        \n",
    "    return meta\n",
    "\n",
    "@app.delete(\"/api/projects/{project_name}\")\n",
    "def delete_project(project_name: str):\n",
    "    path = get_project_path(project_name)\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(status_code=404, detail=\"Project not found\")\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        return {\"status\": \"deleted\", \"name\": project_name}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Gravity Video Editor Backend\"}\n",
    "\n",
    "@app.post(\"/upload-batch/\")\n",
    "async def upload_batch(\n",
    "    files: List[UploadFile] = File(...),\n",
    "    project_name: Optional[str] = Form(None)\n",
    "):\n",
    "    uploaded_files = []\n",
    "    \n",
    "    # Determine target directory\n",
    "    if project_name:\n",
    "        # Sanitize!\n",
    "        safe_name = \"\".join(c for c in project_name if c.isalnum() or c in (' ', '_', '-')).strip()\n",
    "        target_dir = os.path.join(get_project_path(safe_name), \"source_media\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir) # Auto create if slightly out of sync\n",
    "        # Also ensure simple mount access via /projects/name/source_media\n",
    "        web_path_prefix = f\"/projects/{project_name}/source_media\"\n",
    "    else:\n",
    "        target_dir = UPLOAD_DIR\n",
    "        web_path_prefix = \"/uploads\"\n",
    "\n",
    "    print(f\"Received {len(files)} files. Project: {project_name or 'None (Legacy)'}\")\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            file_path = os.path.join(target_dir, file.filename)\n",
    "            with open(file_path, \"wb\") as buffer:\n",
    "                shutil.copyfileobj(file.file, buffer)\n",
    "            \n",
    "            try:\n",
    "                # Try getting video duration (opencv)\n",
    "                duration = get_video_duration(file_path)\n",
    "            except:\n",
    "                duration = 0\n",
    "            \n",
    "            # If it's 0 and looks like audio, we can try moviepy later or just leave it\n",
    "            # For now 0 is fine, Frontend handles it (defaulting to 5s visual or looping music)\n",
    "            \n",
    "            uploaded_files.append({\n",
    "                \"name\": file.filename,\n",
    "                \"path\": f\"{web_path_prefix}/{file.filename}\",\n",
    "                \"duration\": duration\n",
    "            })\n",
    "            print(f\"Successfully uploaded: {file.filename} ({duration}s) to {target_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {file.filename}: {str(e)}\")\n",
    "            pass\n",
    "            \n",
    "    return {\"files\": uploaded_files}\n",
    "\n",
    "@app.get(\"/uploaded-videos/\")\n",
    "def list_uploaded_videos(project_name: Optional[str] = None):\n",
    "    # This might need to support project scoping\n",
    "    videos = []\n",
    "    \n",
    "    if project_name:\n",
    "        target_dir = os.path.join(get_project_path(project_name), \"source_media\")\n",
    "        web_path_prefix = f\"/projects/{project_name}/source_media\"\n",
    "    else:\n",
    "        target_dir = UPLOAD_DIR\n",
    "        web_path_prefix = \"/uploads\"\n",
    "        \n",
    "    if os.path.exists(target_dir):\n",
    "        for filename in os.listdir(target_dir):\n",
    "            if filename.lower().endswith(('.mp4', '.mov', '.avi', '.mkv')):\n",
    "                file_path = os.path.join(target_dir, filename)\n",
    "                duration = get_video_duration(file_path)\n",
    "                videos.append({\n",
    "                    \"name\": filename,\n",
    "                    \"path\": f\"{web_path_prefix}/{filename}\",\n",
    "                    \"duration\": duration\n",
    "                })\n",
    "    return {\"files\": videos}\n",
    "\n",
    "@app.get(\"/uploaded-audio/\")\n",
    "def list_uploaded_audio(project_name: Optional[str] = None):\n",
    "    audio_files = []\n",
    "    \n",
    "    if project_name:\n",
    "        target_dir = os.path.join(get_project_path(project_name), \"source_media\")\n",
    "        web_path_prefix = f\"/projects/{project_name}/source_media\"\n",
    "    else:\n",
    "        target_dir = UPLOAD_DIR\n",
    "        web_path_prefix = \"/uploads\"\n",
    "        \n",
    "    if os.path.exists(target_dir):\n",
    "        for filename in os.listdir(target_dir):\n",
    "            if filename.lower().endswith(('.mp3', '.wav', '.aac', '.m4a')):\n",
    "                file_path = os.path.join(target_dir, filename)\n",
    "                # duration = get_audio_duration(file_path) # Future\n",
    "                audio_files.append({\n",
    "                    \"name\": filename,\n",
    "                    \"path\": f\"{web_path_prefix}/{filename}\",\n",
    "                    \"type\": \"audio\"\n",
    "                })\n",
    "    return {\"files\": audio_files}\n",
    "\n",
    "class AnalyzeRequest(BaseModel):\n",
    "    project_name: str\n",
    "    file_names: List[str]\n",
    "    description: Optional[str] = None\n",
    "    api_key: Optional[str] = None\n",
    "\n",
    "\n",
    "@app.post(\"/analyze/\")\n",
    "async def analyze_project(request: AnalyzeRequest):\n",
    "    # Determine paths based on whether project exists in new structure\n",
    "    project_path = get_project_path(request.project_name)\n",
    "    \n",
    "    # Force output to project directory\n",
    "    if not os.path.exists(project_path):\n",
    "        os.makedirs(project_path, exist_ok=True)\n",
    "    output_dir = project_path\n",
    "\n",
    "    # PERSIST DESCRIPTION\n",
    "    if request.description:\n",
    "        manifest_path = os.path.join(project_path, \"project.json\")\n",
    "        meta = {}\n",
    "        if os.path.exists(manifest_path):\n",
    "            try:\n",
    "                with open(manifest_path, 'r') as f: meta = json.load(f)\n",
    "            except: pass\n",
    "        \n",
    "        meta[\"description\"] = request.description\n",
    "        # ensure other fields exist if creating new\n",
    "        if \"name\" not in meta: meta[\"name\"] = request.project_name\n",
    "        if \"created_at\" not in meta: meta[\"created_at\"] = datetime.now().isoformat()\n",
    "        \n",
    "        with open(manifest_path, \"w\") as f:\n",
    "            json.dump(meta, f)\n",
    "\n",
    "    if os.path.exists(os.path.join(project_path, \"source_media\")):\n",
    "        source_dir = os.path.join(project_path, \"source_media\")\n",
    "    else:\n",
    "        # Fallback to uploads if not in new structure yet\n",
    "        source_dir = UPLOAD_DIR\n",
    "\n",
    "    # Construct full paths\n",
    "    video_paths = [os.path.join(source_dir, fname) for fname in request.file_names]\n",
    "    \n",
    "    # Verify files exist and fix paths if needed\n",
    "    verified_paths = []\n",
    "    for path in video_paths:\n",
    "        if not os.path.exists(path):\n",
    "             # Fallback check for legacy mixing\n",
    "             legacy_path = os.path.join(UPLOAD_DIR, os.path.basename(path))\n",
    "             if source_dir != UPLOAD_DIR and os.path.exists(legacy_path):\n",
    "                 verified_paths.append(legacy_path)\n",
    "             else:\n",
    "                 raise HTTPException(status_code=404, detail=f\"File not found: {path}\")\n",
    "        else:\n",
    "            verified_paths.append(path)\n",
    "            \n",
    "    video_paths = verified_paths\n",
    "\n",
    "    # Run AI Pipeline via Redis Queue\n",
    "    if not q_analysis:\n",
    "        raise HTTPException(status_code=503, detail=\"Analysis Queue Service Unavailable (Redis)\")\n",
    "\n",
    "    try:\n",
    "        # Enqueue job\n",
    "        job = q_analysis.enqueue(\n",
    "            \"backend.worker.tasks.perform_analysis_task\",\n",
    "            video_paths, \n",
    "            request.project_name, \n",
    "            output_dir, \n",
    "            request.description, \n",
    "            api_key=request.api_key,\n",
    "            job_timeout='30m'\n",
    "        )\n",
    "        \n",
    "        print(f\"DEBUG: Start Analysis Job {job.id}\")\n",
    "        return {\"status\": \"queued\", \"job_id\": job.id, \"message\": \"AI Analysis started\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to enqueue analysis: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Queue Error: {str(e)}\")\n",
    "\n",
    "@app.get(\"/analysis-status/{job_id}\")\n",
    "async def get_analysis_status(job_id: str):\n",
    "    if not redis_conn:\n",
    "        raise HTTPException(status_code=503, detail=\"Redis Unavailable\")\n",
    "        \n",
    "    try:\n",
    "        job = Job.fetch(job_id, connection=redis_conn)\n",
    "        status = job.get_status()\n",
    "        \n",
    "        # Build response from meta + status\n",
    "        response = job.meta\n",
    "        response[\"status\"] = status\n",
    "        \n",
    "        # Map RQ statuses to API statuses\n",
    "        if status == \"started\": response[\"status\"] = \"processing\"\n",
    "        if status == \"finished\": response[\"status\"] = \"completed\"\n",
    "        \n",
    "        return response\n",
    "    except NoSuchJobError:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/projects/{project_name}/edl\")\n",
    "async def get_project_edl(project_name: str):\n",
    "    # Check new project struct ONLY\n",
    "    project_path = get_project_path(project_name)\n",
    "    file_path = os.path.join(project_path, f\"{project_name}.xml\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return FileResponse(file_path, media_type='text/xml', filename=f\"{project_name}.xml\")\n",
    "        \n",
    "    return JSONResponse(status_code=404, content={\"detail\": f\"EDL not found at {file_path}\", \"cwd\": os.getcwd()})\n",
    "\n",
    "@app.get(\"/api/projects/{project_name}/analysis\")\n",
    "async def get_project_analysis(project_name: str):\n",
    "    project_path = get_project_path(project_name)\n",
    "    file_path = os.path.join(project_path, f\"{project_name}_analysis.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return FileResponse(file_path, media_type='application/json')\n",
    "    return JSONResponse(status_code=404, content={\"detail\": \"Analysis not found yet\"})\n",
    "\n",
    "class RegenerateRequest(BaseModel):\n",
    "    instruction: Optional[str] = None\n",
    "    api_key: Optional[str] = None\n",
    "\n",
    "@app.post(\"/api/projects/{project_name}/regenerate-xml\")\n",
    "async def regenerate_project_xml(project_name: str, request: Optional[RegenerateRequest] = None):\n",
    "    project_path = get_project_path(project_name)\n",
    "    analysis_path = os.path.join(project_path, f\"{project_name}_analysis.json\")\n",
    "    output_xml_path = os.path.join(project_path, f\"{project_name}.xml\")\n",
    "    \n",
    "    if not os.path.exists(analysis_path):\n",
    "         # Try legacy Uploads path\n",
    "         legacy_path = os.path.join(UPLOAD_DIR, f\"{project_name}_analysis.json\")\n",
    "         if os.path.exists(legacy_path):\n",
    "             analysis_path = legacy_path\n",
    "         else:\n",
    "             raise HTTPException(status_code=404, detail=\"Analysis file not found. Please run analysis first.\")\n",
    "\n",
    "    try:\n",
    "        with open(analysis_path, 'r') as f:\n",
    "            project_data = json.load(f)\n",
    "            \n",
    "        # Get Description from project.json if available to pass as context\n",
    "        user_description = None\n",
    "        \n",
    "        # 1. Prefer explicit instruction from request\n",
    "        if request and request.instruction:\n",
    "            user_description = request.instruction\n",
    "        else:\n",
    "            # 2. Fallback to stored project description\n",
    "            manifest_path = os.path.join(project_path, \"project.json\")\n",
    "            if os.path.exists(manifest_path):\n",
    "                with open(manifest_path, 'r') as f:\n",
    "                     meta = json.load(f)\n",
    "                     user_description = meta.get(\"description\")\n",
    "        \n",
    "        api_key_to_use = request.api_key if request else None\n",
    "\n",
    "        # Regenerate\n",
    "        success = ai_engine.generate_xml_edl(project_data, output_xml_path, project_name, user_description=user_description, api_key=api_key_to_use)\n",
    "        \n",
    "        if not success:\n",
    "            raise HTTPException(status_code=500, detail=\"AI Generation Failed. Fallback XML was generated, but AI features (shorts, overlays) are missing. Please check server logs (likely missing GEMINI_API_KEY).\")\n",
    "\n",
    "        return {\"status\": \"success\", \"message\": \"XML Regenerated\", \"path\": output_xml_path}\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "class ExportRequest(BaseModel):\n",
    "    project: dict \n",
    "\n",
    "@app.post(\"/export-video/\")\n",
    "async def export_video(request: ExportRequest):\n",
    "    if not q_render:\n",
    "         return {\"error\": \"Render Service Unavailable (Redis Queue)\"}\n",
    "    \n",
    "    try:\n",
    "        # Enqueue job\n",
    "        job = q_render.enqueue(\n",
    "            \"backend.worker.tasks.perform_export_task\",\n",
    "            request.project, \n",
    "            EXPORT_DIR,\n",
    "            job_timeout='1h'\n",
    "        )\n",
    "        \n",
    "        # DEBUG: Log payload\n",
    "        try:\n",
    "            debug_path = os.path.join(os.getcwd(), \"debug_payload.log\")\n",
    "            with open(debug_path, \"w\") as f:\n",
    "                clips = request.project.get('clips', [])\n",
    "                f.write(f\"Export Job Enqueued: {job.id}\\n\")\n",
    "                f.write(f\"Total Clips: {len(clips)}\\n\")\n",
    "        except: pass\n",
    "\n",
    "        return {\"status\": \"queued\", \"job_id\": job.id}\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to enqueue export: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/export-status/{job_id}\")\n",
    "async def get_export_status(job_id: str):\n",
    "    if not redis_conn:\n",
    "        raise HTTPException(status_code=503, detail=\"Redis Unavailable\")\n",
    "        \n",
    "    try:\n",
    "        job = Job.fetch(job_id, connection=redis_conn)\n",
    "        status = job.get_status()\n",
    "        \n",
    "        response = job.meta or {}\n",
    "        response[\"status\"] = status\n",
    "        \n",
    "        if status == \"started\": response[\"status\"] = \"processing\"\n",
    "        if status == \"finished\": response[\"status\"] = \"completed\"\n",
    "        if status == \"failed\": response[\"status\"] = \"failed\"\n",
    "        \n",
    "        return response\n",
    "    except NoSuchJobError:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/cancel-export/{job_id}\")\n",
    "async def cancel_export_job(job_id: str):\n",
    "    if not redis_conn:\n",
    "        raise HTTPException(status_code=503, detail=\"Redis Unavailable\")\n",
    "        \n",
    "    try:\n",
    "        job = Job.fetch(job_id, connection=redis_conn)\n",
    "        job.cancel()\n",
    "        return {\"status\": \"cancelling\", \"message\": \"Job cancellation requested\"}\n",
    "    except NoSuchJobError:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/projects/{project_name}/chat-history\")\n",
    "async def get_project_chat_history(project_name: str):\n",
    "    project_path = get_project_path(project_name)\n",
    "    history_file = os.path.join(project_path, \"chat_history.json\")\n",
    "    \n",
    "    if os.path.exists(history_file):\n",
    "        try:\n",
    "            with open(history_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return data\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    query: str\n",
    "    project_name: str = None\n",
    "    api_key: Optional[str] = None\n",
    "    current_state: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "@app.post(\"/chat/\")\n",
    "async def chat_with_ai(request: ChatRequest):\n",
    "    context = None\n",
    "    project_path = None\n",
    "    \n",
    "    # Determine Project Path and Context\n",
    "    if request.project_name:\n",
    "         # Standard Project Path\n",
    "         p_path = get_project_path(request.project_name)\n",
    "         project_path = p_path\n",
    "         \n",
    "         analysis_path = os.path.join(p_path, f\"{request.project_name}_analysis.json\")\n",
    "         # Legacy fallback\n",
    "         if not os.path.exists(analysis_path):\n",
    "             legacy_path = os.path.join(UPLOAD_DIR, f\"{request.project_name}_analysis.json\")\n",
    "             if os.path.exists(legacy_path):\n",
    "                 analysis_path = legacy_path\n",
    "             \n",
    "         if os.path.exists(analysis_path):\n",
    "             try:\n",
    "                 with open(analysis_path, 'r') as f:\n",
    "                     context = json.load(f)\n",
    "             except Exception as e:\n",
    "                 print(f\"Failed to load analysis for chat context: {e}\")\n",
    "    else:\n",
    "        # Default global chat\n",
    "        project_path = os.path.join(PROJECTS_DIR, \"_global_chat\")\n",
    "\n",
    "    # Delegate to Chat Engine (handles LangChain history internally)\n",
    "    # Pass current_state if provided by frontend\n",
    "    response = chat_engine.chat(\n",
    "        request.query, \n",
    "        context, \n",
    "        project_path=project_path, \n",
    "        api_key=request.api_key,\n",
    "        current_state=request.current_state\n",
    "    )\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    # uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "\n",
    "**Metrics:**\n",
    "1.  **Transcription Accuracy:** Comparison of 'dirty' vs 'sanitized' transcripts.\n",
    "2.  **Clip Retention Rate:** The percentage of original footage kept vs. discarded (Target: 30-40% for concise edits).\n",
    "3.  **Render Integrity:** Success rate of ffmpeg exports without sync issues.\n",
    "\n",
    "**Sample Output Analysis:**\n",
    "The system successfully generates an XML EDL (Edit Decision List) which includes:\n",
    "- `keep='false'` flags for bad audio or silence.\n",
    "- Color correction tags `<correction type='brightness' value='1.3' />` for dark clips.\n",
    "- Viral overlay definitions positioned at semantic 'punchlines'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations\n",
    "\n",
    "**Bias in Editing:**\n",
    "Automated editing based on 'meaningful speech' carries the risk of erasing hesitations or dialect features that are culturally significant. The prompt explicitly instructs to preserve natural flow but remove 'broken' grammar, which requires careful tuning to avoid linguistic discrimination.\n",
    "\n",
    "**Content Moderation:**\n",
    "The system currently processes all inputs. Future improvements must include safety filters (using Gemini's safety settings) to prevent the automated enhancement of harmful or violent content.\n",
    "\n",
    "**Responsible Use:**\n",
    "the 'Deepfake' risk is mitigated by only editing *existing* footage rather than generating new pixel data (Video-to-Video), ensuring the subject's likeness remains authentic, merely re-sequenced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Future Scope\n",
    "\n",
    "**Conclusion:**\n",
    "Gravity Edits demonstrates a viable pipeline for autonomous video post-production. By combining deterministic computer vision metrics with the semantic reasoning of LLMs, we achieve a 'best of both worlds' approach to editing.\n",
    "\n",
    "**Future Scope:**\n",
    "1.  **Multi-Modal Agents:** allowing the AI to 'watch' the video (Video-Input LLMs) rather than just reading transcripts.\n",
    "2.  **Audio Ducking:** Intelligent background music volume automation.\n",
    "3.  **Style Transfer:** Using GenAI to apply artistic styles to specific clips.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}